{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5f98b5f",
      "metadata": {},
      "source": [
        "# Zanim puścimy komórki kodu poniżej \n",
        "\n",
        "Tutaj znajduje się skrypt SLURM, jakiego ja używam aby włączyć sobie sesję Jupyter lokalnie na Atenie:\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition plgrid-gpu-a100\n",
        "#SBATCH --nodes 1\n",
        "#SBATCH --ntasks-per-node 96\n",
        "#SBATCH --gres=gpu:1\n",
        "#SBATCH --time 15:30:00\n",
        "#SBATCH --job-name jupyter-notebook-tunnel\n",
        "#SBATCH --output jupyter-log-%J.txt\n",
        "#SBATCH --chdir=\"/net/tscratch/people/plgdusza/Sussex Dataset\"\n",
        "\n",
        "## get tunneling info\n",
        "XDG_RUNTIME_DIR=\"\"\n",
        "ipnport=$(shuf -i8000-9999 -n1)\n",
        "ipnip=$(hostname -i)\n",
        "user=$USER\n",
        "\n",
        "cd \"/net/tscratch/people/plgdusza/Sussex Dataset\" || { \n",
        "    echo \"ERROR: Could not cd to scratch at /net/tscratch/people/plgdusza/Sussex Dataset\" >&2\n",
        "    exit 1\n",
        "}\n",
        "\n",
        "module load GCCcore/13.2.0 Python/3.11.5\n",
        "python3.11 -m venv venv                            # create venv under scratch\n",
        "source venv/bin/activate                           # activate venv\n",
        "\n",
        "pip install --upgrade pip                          # update pip in venv \n",
        "pip install jupyterlab ipykernel torch==2.2.2+cu121 \\\n",
        "  torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "#pip install jupyterlab ipykernel torch==2.0.1+cu117 \\\n",
        "#    torchvision==0.15.2+cu117 torchaudio==2.0.2\n",
        "\n",
        "\n",
        "module load GCC/11.3.0\n",
        "module load NVHPC/22.11-CUDA-11.7.0\n",
        "module load OpenMPI/4.1.4\n",
        "module load TensorFlow/2.11.0-CUDA-11.7.0\n",
        "module load JupyterLab/3.5.0\n",
        "\n",
        "\n",
        "\n",
        "## print tunneling instructions to jupyter-log-{jobid}.txt\n",
        "echo -e \"\n",
        "    Copy/Paste this in your local terminal to ssh tunnel with remote\n",
        "    -----------------------------------------------------------------\n",
        "    ssh -o ServerAliveInterval=300 -N -L $ipnport:$ipnip:$ipnport ${user}@athena.cyfronet.pl\n",
        "    -----------------------------------------------------------------\n",
        " \n",
        "    Then open a browser on your local machine to the following address\n",
        "    ------------------------------------------------------------------\n",
        "    localhost:$ipnport  (prefix w/ https:// if using password)\n",
        "    ------------------------------------------------------------------\n",
        "    \"\n",
        "## start an ipcluster instance and launch jupyter server\n",
        "jupyter lab --no-browser --port=$ipnport --ip=$ipnip\n",
        "\n",
        "\n",
        "echo \"=== Deactivating and removing virtual environment ===\"\n",
        "deactivate\n",
        "```\n",
        "\n",
        "Skrypt ten będzie tworzyć plik txt z kodem ssh do Ateny, oraz późniejszym linkiem http do Colaba u mnie na scratchu w folderze Sussex, w celu zmiany lokalizacji pojawienia się pliku txt, trzeba zmienić tą linijkę kodu ze swoją ścieżką:\n",
        "\n",
        "```\n",
        "cd \"/net/tscratch/people/plgdusza/Sussex Dataset\" || { \n",
        "    echo \"ERROR: Could not cd to scratch at /net/tscratch/people/plgdusza/Sussex Dataset\" >&2\n",
        "    exit 1\n",
        "}\n",
        "```\n",
        "\n",
        "Ale ponieważ rozumiem macie dostęp do mojego scratcha, nie jest to potrzebne, tylko wiedzcie żeby szukać txt w **/net/tscratch/people/plgdusza/Sussex Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be55395",
      "metadata": {},
      "source": [
        "**UWAGA, skrypt SLURM powyżej jest na 15h**, należy to dostosować do czasu jaki mniej więcej kod poniżej uczenia modeli się będzie wykonywać aby nie stracić modelu i żeby nie marnować resourców"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V0SiMotH3PRQ",
      "metadata": {
        "id": "V0SiMotH3PRQ"
      },
      "source": [
        "# Sprawdzanie importów na venvie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d967ab-9fef-413f-811c-8cf2645d6465",
      "metadata": {
        "id": "89d967ab-9fef-413f-811c-8cf2645d6465",
        "outputId": "b9ce74da-7590-46a3-fef5-bbbc63eb567a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jupyterlab 3.5.0: 17.81 MB\n",
            "----------------------------------------\n",
            "json5 0.9.10: 0.13 MB\n",
            "----------------------------------------\n",
            "nbclassic 0.4.8: 36.04 MB\n",
            "----------------------------------------\n",
            "anyio 3.6.1: 0.58 MB\n",
            "----------------------------------------\n",
            "sniffio 1.3.0: 0.01 MB\n",
            "----------------------------------------\n",
            "ipywidgets 7.6.3: 0.60 MB\n",
            "----------------------------------------\n",
            "ipykernel 6.13.0: 0.70 MB\n",
            "----------------------------------------\n",
            "nbconvert 6.5.3: 1.46 MB\n",
            "----------------------------------------\n",
            "notebook 6.4.0: 34.68 MB\n",
            "----------------------------------------\n",
            "tornado 6.2: 2.91 MB\n",
            "----------------------------------------\n",
            "nbformat 5.4.0: 0.39 MB\n",
            "----------------------------------------\n",
            "traitlets 5.2.0: 0.82 MB\n",
            "----------------------------------------\n",
            "bleach 5.0.1: 1.15 MB\n",
            "----------------------------------------\n",
            "widgetsnbextension 3.5.1: 4.61 MB\n",
            "----------------------------------------\n",
            "fastjsonschema 2.16.1: 0.14 MB\n",
            "----------------------------------------\n",
            "asttokens 2.0.8: 0.08 MB\n",
            "----------------------------------------\n",
            "debugpy 1.4.1: 24.82 MB\n",
            "----------------------------------------\n",
            "tinycss2 1.1.1: 0.14 MB\n",
            "----------------------------------------\n",
            "executing 1.0.0: 0.07 MB\n",
            "----------------------------------------\n",
            "jedi 0.18.1: 4.98 MB\n",
            "----------------------------------------\n",
            "terminado 0.13.0: 0.06 MB\n",
            "----------------------------------------\n",
            "parso 0.8.3: 0.53 MB\n",
            "----------------------------------------\n",
            "defusedxml 0.7.1: 0.06 MB\n",
            "----------------------------------------\n",
            "nbclient 0.6.3: 0.31 MB\n",
            "----------------------------------------\n",
            "testpath 0.6.0: 0.18 MB\n",
            "----------------------------------------\n",
            "backcall 0.2.0: 0.05 MB\n",
            "----------------------------------------\n",
            "soupsieve 2.3.1: 0.23 MB\n",
            "----------------------------------------\n",
            "lxml 4.9.1: 17.38 MB\n",
            "----------------------------------------\n",
            "regex 2022.4.24: 2.46 MB\n",
            "----------------------------------------\n",
            "fsspec 2022.3.0: 0.84 MB\n",
            "----------------------------------------\n",
            "pytz 2022.1: 0.97 MB\n",
            "----------------------------------------\n",
            "certifi 2021.10.8: 0.27 MB\n",
            "----------------------------------------\n",
            "setuptools 62.1.0: 4.54 MB\n",
            "----------------------------------------\n",
            "cryptography 37.0.1: 5.15 MB\n",
            "----------------------------------------\n",
            "pip 24.2: 9.64 MB\n",
            "----------------------------------------\n",
            "keyring 23.5.0: 0.14 MB\n",
            "----------------------------------------\n",
            "attrs 21.4.0: 0.01 MB\n",
            "----------------------------------------\n",
            "virtualenv 20.14.1: 9.45 MB\n",
            "----------------------------------------\n",
            "packaging 20.9: 0.18 MB\n",
            "----------------------------------------\n",
            "click 8.1.3: 0.64 MB\n",
            "----------------------------------------\n",
            "pytest 7.1.2: 0.01 MB\n",
            "----------------------------------------\n",
            "psutil 5.9.0: 1.62 MB\n",
            "----------------------------------------\n",
            "pbr 5.8.1: 0.51 MB\n",
            "----------------------------------------\n",
            "pexpect 4.8.0: 0.32 MB\n",
            "----------------------------------------\n",
            "jsonschema 4.4.0: 0.55 MB\n",
            "----------------------------------------\n",
            "mock 4.0.3: 0.18 MB\n",
            "----------------------------------------\n",
            "chardet 4.0.0: 1.61 MB\n",
            "----------------------------------------\n",
            "simplejson 3.17.6: 0.47 MB\n",
            "----------------------------------------\n",
            "flit 3.7.1: 0.19 MB\n",
            "----------------------------------------\n",
            "filelock 3.6.0: 0.03 MB\n",
            "----------------------------------------\n",
            "idna 3.3: 0.47 MB\n",
            "----------------------------------------\n",
            "bcrypt 3.2.2: 0.12 MB\n",
            "----------------------------------------\n",
            "intervaltree 3.1.0: 0.13 MB\n",
            "----------------------------------------\n",
            "pyparsing 3.0.8: 0.69 MB\n",
            "----------------------------------------\n",
            "requests 2.27.1: 0.33 MB\n",
            "----------------------------------------\n",
            "pycparser 2.21: 1.03 MB\n",
            "----------------------------------------\n",
            "paramiko 2.10.4: 1.26 MB\n",
            "----------------------------------------\n",
            "platformdirs 2.4.1: 0.08 MB\n",
            "----------------------------------------\n",
            "sortedcontainers 2.4.0: 0.23 MB\n",
            "----------------------------------------\n",
            "pathlib2 2.3.7.post1: 0.11 MB\n",
            "----------------------------------------\n",
            "snowballstemmer 2.2.0: 1.32 MB\n",
            "----------------------------------------\n",
            "tomli 2.0.1: 0.05 MB\n",
            "----------------------------------------\n",
            "xlrd 2.0.1: 0.57 MB\n",
            "----------------------------------------\n",
            "urllib3 1.26.9: 0.68 MB\n",
            "----------------------------------------\n",
            "cffi 1.15.0: 0.62 MB\n",
            "----------------------------------------\n",
            "py 1.11.0: 0.55 MB\n",
            "----------------------------------------\n",
            "pkginfo 1.8.2: 0.18 MB\n",
            "----------------------------------------\n",
            "asn1crypto 1.5.1: 0.87 MB\n",
            "----------------------------------------\n",
            "atomicwrites 1.4.0: 0.01 MB\n",
            "----------------------------------------\n",
            "shellingham 1.4.0: 0.02 MB\n",
            "----------------------------------------\n",
            "pylev 1.4.0: 0.01 MB\n",
            "----------------------------------------\n",
            "blist 1.3.6: 0.37 MB\n",
            "----------------------------------------\n",
            "poetry 1.1.13: 3.77 MB\n",
            "----------------------------------------\n",
            "iniconfig 1.1.1: 0.01 MB\n",
            "----------------------------------------\n",
            "html5lib 1.1: 0.94 MB\n",
            "----------------------------------------\n",
            "msgpack 1.0.3: 0.73 MB\n",
            "----------------------------------------\n",
            "pluggy 1.0.0: 0.06 MB\n",
            "----------------------------------------\n",
            "wheel 0.37.1: 0.17 MB\n",
            "----------------------------------------\n",
            "Cython 0.29.28: 16.49 MB\n",
            "----------------------------------------\n",
            "future 0.18.2: 2.45 MB\n",
            "----------------------------------------\n",
            "pyrsistent 0.18.1: 0.29 MB\n",
            "----------------------------------------\n",
            "docutils 0.17.1: 3.26 MB\n",
            "----------------------------------------\n",
            "ecdsa 0.17.0: 0.78 MB\n",
            "----------------------------------------\n",
            "lockfile 0.12.2: 0.05 MB\n",
            "----------------------------------------\n",
            "toml 0.10.2: 0.09 MB\n",
            "----------------------------------------\n",
            "tomlkit 0.10.2: 0.26 MB\n",
            "----------------------------------------\n",
            "pathspec 0.9.0: 0.14 MB\n",
            "----------------------------------------\n",
            "cleo 0.8.1: 0.09 MB\n",
            "----------------------------------------\n",
            "jeepney 0.8.0: 0.27 MB\n",
            "----------------------------------------\n",
            "netaddr 0.8.0: 8.19 MB\n",
            "----------------------------------------\n",
            "alabaster 0.7.12: 0.03 MB\n",
            "----------------------------------------\n",
            "glob2 0.7: 0.04 MB\n",
            "----------------------------------------\n",
            "ptyprocess 0.7.0: 0.06 MB\n",
            "----------------------------------------\n",
            "clikit 0.6.2: 0.53 MB\n",
            "----------------------------------------\n",
            "webencodings 0.5.1: 0.06 MB\n",
            "----------------------------------------\n",
            "pyasn1 0.4.8: 0.62 MB\n",
            "----------------------------------------\n",
            "colorama 0.4.4: 0.05 MB\n",
            "----------------------------------------\n",
            "distlib 0.3.4: 1.42 MB\n",
            "----------------------------------------\n",
            "crashtest 0.3.1: 0.02 MB\n",
            "----------------------------------------\n",
            "editables 0.3: 0.01 MB\n",
            "----------------------------------------\n",
            "cachy 0.3.0: 0.12 MB\n",
            "----------------------------------------\n",
            "wcwidth 0.2.5: 0.45 MB\n",
            "----------------------------------------\n",
            "pastel 0.2.1: 0.02 MB\n",
            "----------------------------------------\n",
            "pytoml 0.1.21: 0.04 MB\n",
            "----------------------------------------\n",
            "total size =  241.9105599999999\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pkg_resources\n",
        "\n",
        "def calc_container(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size\n",
        "\n",
        "\n",
        "dists = list(pkg_resources.working_set)\n",
        "total_size=0\n",
        "for dist in dists:\n",
        "    try:\n",
        "        path = os.path.join(dist.location, dist.project_name)\n",
        "        size = calc_container(path)\n",
        "        # Convert to megabytes and print with two decimal places\n",
        "        size_mb = size / 1_000_000\n",
        "        total_size+=size_mb\n",
        "        if size_mb > 0.001:\n",
        "            print(f\"{dist}: {size_mb:.2f} MB\")\n",
        "            print(\"-\" * 40)\n",
        "    except OSError:\n",
        "        print(f\"{dist.project_name} no longer exists\")\n",
        "\n",
        "print(\"total size = \", total_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g43xJw8u3hjS",
      "metadata": {
        "id": "g43xJw8u3hjS"
      },
      "source": [
        "# Jeśli macie dostęp do mojego scratcha to **nie róbcie** tego tutaj w tej sekcji\n",
        "\n",
        "Jak macie dane idziemy dalej, ponieważ te tutaj kody zajmują 30 minut, one już zostały spreparowane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c99360e0-2f82-4b65-95b9-b21235229837",
      "metadata": {
        "id": "c99360e0-2f82-4b65-95b9-b21235229837"
      },
      "outputs": [],
      "source": [
        "#Creating a Big Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d78496d-1d18-499a-9d8a-c8f56a7c06b6",
      "metadata": {
        "id": "7d78496d-1d18-499a-9d8a-c8f56a7c06b6",
        "outputId": "30af89fd-2dbe-470d-8ff7-57991e10a47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[train/Bag] labels rows = 196072\n",
            "  Read Acc_x.txt → (196072, 500)\n",
            "  Read Acc_y.txt → (196072, 500)\n",
            "  Read Acc_z.txt → (196072, 500)\n",
            "  Read Gyr_x.txt → (196072, 500)\n",
            "  Read Gyr_y.txt → (196072, 500)\n",
            "  Read Gyr_z.txt → (196072, 500)\n",
            "  Read Mag_x.txt → (196072, 500)\n",
            "  Read Mag_y.txt → (196072, 500)\n",
            "  Read Mag_z.txt → (196072, 500)\n",
            "  Read Acc_rot_x.txt → (196072, 500)\n",
            "  Read Acc_rot_y.txt → (196072, 500)\n",
            "  Read Acc_rot_z.txt → (196072, 500)\n",
            "  Read Acc_solo_mag.txt → (196072, 500)\n",
            "  Stacked shape: (196072, 500, 13)\n",
            "\n",
            "[train/Hand] labels rows = 196072\n",
            "  Read Acc_x.txt → (196072, 500)\n",
            "  Read Acc_y.txt → (196072, 500)\n",
            "  Read Acc_z.txt → (196072, 500)\n",
            "  Read Gyr_x.txt → (196072, 500)\n",
            "  Read Gyr_y.txt → (196072, 500)\n",
            "  Read Gyr_z.txt → (196072, 500)\n",
            "  Read Mag_x.txt → (196072, 500)\n",
            "  Read Mag_y.txt → (196072, 500)\n",
            "  Read Mag_z.txt → (196072, 500)\n",
            "  Read Acc_rot_x.txt → (196072, 500)\n",
            "  Read Acc_rot_y.txt → (196072, 500)\n",
            "  Read Acc_rot_z.txt → (196072, 500)\n",
            "  Read Acc_solo_mag.txt → (196072, 500)\n",
            "  Stacked shape: (196072, 500, 13)\n",
            "\n",
            "[train/Hips] labels rows = 196072\n",
            "  Read Acc_x.txt → (196072, 500)\n",
            "  Read Acc_y.txt → (196072, 500)\n",
            "  Read Acc_z.txt → (196072, 500)\n",
            "  Read Gyr_x.txt → (196072, 500)\n",
            "  Read Gyr_y.txt → (196072, 500)\n",
            "  Read Gyr_z.txt → (196072, 500)\n",
            "  Read Mag_x.txt → (196072, 500)\n",
            "  Read Mag_y.txt → (196072, 500)\n",
            "  Read Mag_z.txt → (196072, 500)\n",
            "  Read Acc_rot_x.txt → (196072, 500)\n",
            "  Read Acc_rot_y.txt → (196072, 500)\n",
            "  Read Acc_rot_z.txt → (196072, 500)\n",
            "  Read Acc_solo_mag.txt → (196072, 500)\n",
            "  Stacked shape: (196072, 500, 13)\n",
            "\n",
            "[train/Torso] labels rows = 196072\n",
            "  Read Acc_x.txt → (196072, 500)\n",
            "  Read Acc_y.txt → (196072, 500)\n",
            "  Read Acc_z.txt → (196072, 500)\n",
            "  Read Gyr_x.txt → (196072, 500)\n",
            "  Read Gyr_y.txt → (196072, 500)\n",
            "  Read Gyr_z.txt → (196072, 500)\n",
            "  Read Mag_x.txt → (196072, 500)\n",
            "  Read Mag_y.txt → (196072, 500)\n",
            "  Read Mag_z.txt → (196072, 500)\n",
            "  Read Acc_rot_x.txt → (196072, 500)\n",
            "  Read Acc_rot_y.txt → (196072, 500)\n",
            "  Read Acc_rot_z.txt → (196072, 500)\n",
            "  Read Acc_solo_mag.txt → (196072, 500)\n",
            "  Stacked shape: (196072, 500, 13)\n",
            "\n",
            "[validation/Bag] labels rows = 28789\n",
            "  Read Acc_x.txt → (28789, 500)\n",
            "  Read Acc_y.txt → (28789, 500)\n",
            "  Read Acc_z.txt → (28789, 500)\n",
            "  Read Gyr_x.txt → (28789, 500)\n",
            "  Read Gyr_y.txt → (28789, 500)\n",
            "  Read Gyr_z.txt → (28789, 500)\n",
            "  Read Mag_x.txt → (28789, 500)\n",
            "  Read Mag_y.txt → (28789, 500)\n",
            "  Read Mag_z.txt → (28789, 500)\n",
            "  Read Acc_rot_x.txt → (28789, 500)\n",
            "  Read Acc_rot_y.txt → (28789, 500)\n",
            "  Read Acc_rot_z.txt → (28789, 500)\n",
            "  Read Acc_solo_mag.txt → (28789, 500)\n",
            "  Stacked shape: (28789, 500, 13)\n",
            "\n",
            "[validation/Hand] labels rows = 28789\n",
            "  Read Acc_x.txt → (28789, 500)\n",
            "  Read Acc_y.txt → (28789, 500)\n",
            "  Read Acc_z.txt → (28789, 500)\n",
            "  Read Gyr_x.txt → (28789, 500)\n",
            "  Read Gyr_y.txt → (28789, 500)\n",
            "  Read Gyr_z.txt → (28789, 500)\n",
            "  Read Mag_x.txt → (28789, 500)\n",
            "  Read Mag_y.txt → (28789, 500)\n",
            "  Read Mag_z.txt → (28789, 500)\n",
            "  Read Acc_rot_x.txt → (28789, 500)\n",
            "  Read Acc_rot_y.txt → (28789, 500)\n",
            "  Read Acc_rot_z.txt → (28789, 500)\n",
            "  Read Acc_solo_mag.txt → (28789, 500)\n",
            "  Stacked shape: (28789, 500, 13)\n",
            "\n",
            "[validation/Hips] labels rows = 28789\n",
            "  Read Acc_x.txt → (28789, 500)\n",
            "  Read Acc_y.txt → (28789, 500)\n",
            "  Read Acc_z.txt → (28789, 500)\n",
            "  Read Gyr_x.txt → (28789, 500)\n",
            "  Read Gyr_y.txt → (28789, 500)\n",
            "  Read Gyr_z.txt → (28789, 500)\n",
            "  Read Mag_x.txt → (28789, 500)\n",
            "  Read Mag_y.txt → (28789, 500)\n",
            "  Read Mag_z.txt → (28789, 500)\n",
            "  Read Acc_rot_x.txt → (28789, 500)\n",
            "  Read Acc_rot_y.txt → (28789, 500)\n",
            "  Read Acc_rot_z.txt → (28789, 500)\n",
            "  Read Acc_solo_mag.txt → (28789, 500)\n",
            "  Stacked shape: (28789, 500, 13)\n",
            "\n",
            "[validation/Torso] labels rows = 28789\n",
            "  Read Acc_x.txt → (28789, 500)\n",
            "  Read Acc_y.txt → (28789, 500)\n",
            "  Read Acc_z.txt → (28789, 500)\n",
            "  Read Gyr_x.txt → (28789, 500)\n",
            "  Read Gyr_y.txt → (28789, 500)\n",
            "  Read Gyr_z.txt → (28789, 500)\n",
            "  Read Mag_x.txt → (28789, 500)\n",
            "  Read Mag_y.txt → (28789, 500)\n",
            "  Read Mag_z.txt → (28789, 500)\n",
            "  Read Acc_rot_x.txt → (28789, 500)\n",
            "  Read Acc_rot_y.txt → (28789, 500)\n",
            "  Read Acc_rot_z.txt → (28789, 500)\n",
            "  Read Acc_solo_mag.txt → (28789, 500)\n",
            "  Stacked shape: (28789, 500, 13)\n",
            "\n",
            "Final X_all shape: (899444, 500, 13)\n",
            "Final y_all shape: (899444,)\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "\n",
        "# —————————————————————\n",
        "# 1. CONFIG\n",
        "# —————————————————————\n",
        "BASE_4LOC   = '/net/tscratch/people/plgdusza/Sussex Dataset'\n",
        "BASE_FUSION = '/net/tscratch/people/plgdusza/Sussex Dataset/sensor_fusion'\n",
        "splits      = ['train', 'validation']\n",
        "subdirs     = ['Bag', 'Hand', 'Hips', 'Torso']\n",
        "TIMESTEPS   = 500\n",
        "\n",
        "X_list, y_list = [], []\n",
        "\n",
        "# —————————————————————\n",
        "# 2. LOOP OVER SPLITS & LOCATIONS\n",
        "# —————————————————————\n",
        "for split in splits:\n",
        "    for loc in subdirs:\n",
        "        # paths\n",
        "        path_4loc   = os.path.join(BASE_4LOC,   split, loc)\n",
        "        path_fusion = os.path.join(BASE_FUSION, split, loc)\n",
        "\n",
        "        # 2a) Read Label.txt once\n",
        "        lbl1 = os.path.join(path_4loc,   'Label.txt')\n",
        "        lbl2 = os.path.join(path_fusion, 'Label.txt')\n",
        "        label_fp = lbl1 if os.path.exists(lbl1) else lbl2\n",
        "        if not os.path.exists(label_fp):\n",
        "            raise FileNotFoundError(f\"No Label.txt in {path_4loc} or {path_fusion}\")\n",
        "\n",
        "        # load label lines\n",
        "        with open(label_fp, 'r') as f:\n",
        "            lbl_lines = f.read().splitlines()\n",
        "\n",
        "        # compute per-row mode (most frequent value)\n",
        "        modes = []\n",
        "        for line in lbl_lines:\n",
        "            if not line.strip():\n",
        "                # blank line → treat as NaN label\n",
        "                modes.append(np.nan)\n",
        "            else:\n",
        "                vals = [int(x) for x in line.split()]\n",
        "                modes.append(int(np.bincount(vals).argmax()))\n",
        "        modes     = np.array(modes)            # shape: (n_windows,)\n",
        "        n_windows = len(modes)\n",
        "        print(f\"\\n[{split}/{loc}] labels rows = {n_windows}\")\n",
        "\n",
        "        # 2b) Gather sensor files (exclude the Label.txt itself)\n",
        "        files_9  = sorted(glob.glob(os.path.join(path_4loc,   '*.txt')))\n",
        "        files_4  = sorted(glob.glob(os.path.join(path_fusion, '*.txt')))\n",
        "        all_txt  = files_9 + files_4\n",
        "        sensor_files = [fp for fp in all_txt\n",
        "                        if os.path.basename(fp).lower() != 'label.txt']\n",
        "\n",
        "        if len(sensor_files) != 13:\n",
        "            print(f\"Warning: expected 13 sensor files, found {len(sensor_files)}\")\n",
        "\n",
        "        # 2c) Read each sensor file manually\n",
        "        sensor_arrays = []\n",
        "        for fp in sensor_files:\n",
        "            # read raw lines\n",
        "            with open(fp, 'r') as f:\n",
        "                lines = f.read().splitlines()\n",
        "\n",
        "            # parse each line into a float array (or NaNs if blank)\n",
        "            arr_rows = []\n",
        "            for line in lines:\n",
        "                if not line.strip():\n",
        "                    arr_rows.append(np.full((TIMESTEPS,), np.nan))\n",
        "                else:\n",
        "                    row = np.fromstring(line, dtype=float, sep=' ')\n",
        "                    arr_rows.append(row)\n",
        "            arr = np.vstack(arr_rows)  # shape: (n_rows_in_file, TIMESTEPS)\n",
        "            print(f\"  Read {os.path.basename(fp)} → {arr.shape}\")\n",
        "\n",
        "            # pad or truncate at the end to match n_windows\n",
        "            n_rows = arr.shape[0]\n",
        "            if n_rows < n_windows:\n",
        "                pad = np.full((n_windows - n_rows, TIMESTEPS), np.nan)\n",
        "                arr = np.vstack([arr, pad])\n",
        "                print(f\"    → Padded {n_windows-n_rows} rows of NaN\")\n",
        "            elif n_rows > n_windows:\n",
        "                arr = arr[:n_windows, :]\n",
        "                print(f\"    → Truncated to {n_windows} rows\")\n",
        "\n",
        "            sensor_arrays.append(arr)\n",
        "\n",
        "        # 3) Stack channels → (n_windows, TIMESTEPS, 13)\n",
        "        X_loc = np.stack(sensor_arrays, axis=2)\n",
        "        y_loc = modes - 1   # zero-based labels if you need\n",
        "\n",
        "        print(f\"  Stacked shape: {X_loc.shape}\")\n",
        "        X_list.append(X_loc)\n",
        "        y_list.append(y_loc)\n",
        "\n",
        "# —————————————————————\n",
        "# 4. FINAL CONCAT\n",
        "# —————————————————————\n",
        "X_all = np.concatenate(X_list, axis=0)  # → (total_windows, 500, 13)\n",
        "y_all = np.concatenate(y_list, axis=0)  # → (total_windows,)\n",
        "\n",
        "print(f\"\\nFinal X_all shape: {X_all.shape}\")\n",
        "print(f\"Final y_all shape: {y_all.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e3b9da-caf1-48f0-b3d3-a4c24b84399f",
      "metadata": {
        "id": "06e3b9da-caf1-48f0-b3d3-a4c24b84399f"
      },
      "outputs": [],
      "source": [
        "out_path = '/net/tscratch/people/plgdusza/Sussex Dataset/NPZ files data/X_y_data.npz'\n",
        "\n",
        "# this will bundle both X_all and y_all into one compressed archive\n",
        "np.savez(out_path, X_all=X_all, y_all=y_all)\n",
        "print(f\"Saved X_all and y_all to {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uFITevzX3qQW",
      "metadata": {
        "id": "uFITevzX3qQW"
      },
      "source": [
        "# TUTAJ START instalacja wszystkich bibliotek oraz import danych\n",
        "Musicie się upewnić, że nie macie nic na swoim koncie prywatnym na /net/people/plgrid/plg[username] bo inaczej zabraknie wam miejsca.\n",
        "\n",
        "Ilość miejsca zajętego możecie sprawdzić poprzez puszczenie w terminalu na Atenie poprzez ssh plg *username* @athena.cyfronet.pl:\n",
        "\n",
        "```\n",
        "hpc-fs\n",
        "```\n",
        "powinniście zobaczyć coś takiego, gdzie interesuje was ścieżka $HOME, i powinna być prawie pusta, ponieważ instalacja wszystkich bibliotek tutaj potrzebnych zajmuje ponad 7GB.\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6593abb-9c18-4ece-9610-faad8a085227",
      "metadata": {
        "id": "b6593abb-9c18-4ece-9610-faad8a085227"
      },
      "outputs": [],
      "source": [
        "#Start Here - Loading data from saved dataset and spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42316b8-1c51-4b7f-b3f2-c03b165fcbc0",
      "metadata": {
        "id": "e42316b8-1c51-4b7f-b3f2-c03b165fcbc0",
        "outputId": "7a5a900b-b56a-45e4-e8ac-89cc8ccf1c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/net/people/plgrid/plgdusza/.local/lib/python3.10/site-packages/~umpy.libs'.\n",
            "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/net/people/plgrid/plgdusza/.local/lib/python3.10/site-packages/~umpy'.\n",
            "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --q scikit-learn matplotlib seaborn pypots \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e3ba89-fc51-42d0-ba3b-7d8323a0f159",
      "metadata": {
        "id": "b2e3ba89-fc51-42d0-ba3b-7d8323a0f159",
        "outputId": "4dd74365-fa88-43ba-d012-b236b43b4af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.2%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /net/software/v1/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from torch==2.2.2+cu121) (3.6.0)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in /net/software/v1/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from torch==2.2.2+cu121) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /net/software/v1/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from torch==2.2.2+cu121) (2022.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from torchvision==0.17.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.17.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /net/software/v1/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from jinja2->torch==2.2.2+cu121) (2.1.1)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.2+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, triton, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "Successfully installed mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 sympy-1.13.3 torch-2.2.2+cu121 torchaudio-2.2.2+cu121 torchvision-0.17.2+cu121 triton-2.2.0 typing-extensions-4.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --q \\\n",
        "  torch==2.2.2+cu121 \\\n",
        "  torchvision==0.17.2+cu121 \\\n",
        "  torchaudio==2.2.2+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CYcIz77k4vZK",
      "metadata": {
        "id": "CYcIz77k4vZK"
      },
      "source": [
        "Import zbioru danych poniżej, ze scratcha plgdusza\n",
        "\n",
        "Output MUSI wyglądać w ten sposób, a komórka wykona się w ok. 2/3 minuty:\n",
        "\n",
        "```\n",
        "All the files were read\n",
        "Converted to tensor\n",
        "Combined full tensor dataset\n",
        "Simple split compute\n",
        "Split achieved\n",
        "After random_split:\n",
        "  len(train_ds): 629610\n",
        "  len(val_ds):   134916\n",
        "  len(test_ds):  134918\n",
        "One training batch shapes:\n",
        "  xb (inputs)  : torch.Size([32, 500, 13])   # (batch_size, n_steps, n_features)\n",
        "  yb (targets) : torch.Size([32])   # (batch_size,)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43343579-345d-445e-bbe9-762932804155",
      "metadata": {
        "id": "43343579-345d-445e-bbe9-762932804155",
        "outputId": "1875ba09-e5d3-4731-d066-83e7f58b517b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All the files were read\n",
            "Converted to tensor\n",
            "Combined full tensor dataset\n",
            "Simple split compute\n",
            "Split achieved\n",
            "After random_split:\n",
            "  len(train_ds): 629610\n",
            "  len(val_ds):   134916\n",
            "  len(test_ds):  134918\n",
            "One training batch shapes:\n",
            "  xb (inputs)  : torch.Size([32, 500, 13])   # (batch_size, n_steps, n_features)\n",
            "  yb (targets) : torch.Size([32])   # (batch_size,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "data = np.load('/net/tscratch/people/plgdusza/Sussex Dataset/NPZ files data/X_y_data.npz')\n",
        "X_all = data['X_all']\n",
        "y_all = data['y_all']\n",
        "\n",
        "print(\"All the files were read\")\n",
        "\n",
        "\n",
        "# Convert once to tensors\n",
        "X_tensor = torch.from_numpy(X_all).float()\n",
        "y_tensor = torch.from_numpy(y_all).long()\n",
        "X_tensor = X_tensor.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "y_tensor = y_tensor.nan_to_num(nan=0, posinf=0, neginf=0)\n",
        "print(\"Converted to tensor\")\n",
        "\n",
        "full_ds = TensorDataset(X_tensor, y_tensor)\n",
        "print(\"Combined full tensor dataset\")\n",
        "\n",
        "\n",
        "# Compute split sizes\n",
        "n = len(full_ds)\n",
        "n_train = int(0.70 * n)\n",
        "n_val   = int(0.15 * n)\n",
        "n_test  = n - n_train - n_val\n",
        "print(\"Simple split compute\")\n",
        "\n",
        "# This only builds index lists, no data copy\n",
        "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(\"Split achieved\")\n",
        "print(\"After random_split:\")\n",
        "print(f\"  len(train_ds): {len(train_ds)}\")\n",
        "print(f\"  len(val_ds):   {len(val_ds)}\")\n",
        "print(f\"  len(test_ds):  {len(test_ds)}\")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "xb, yb = next(iter(loader))\n",
        "print(\"One training batch shapes:\")\n",
        "print(f\"  xb (inputs)  : {xb.shape}   # (batch_size, n_steps, n_features)\")\n",
        "print(f\"  yb (targets) : {yb.shape}   # (batch_size,)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8A8qMikj47-p",
      "metadata": {
        "id": "8A8qMikj47-p"
      },
      "source": [
        "# Początek Treningu\n",
        "Przykładowy TimesNet\n",
        "\n",
        "Czas na epokę: ok. 10 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e9bc70-4a57-4009-8acf-5ecca34d45e5",
      "metadata": {
        "id": "c5e9bc70-4a57-4009-8acf-5ecca34d45e5",
        "outputId": "dfc0a52b-7961-4be8-8158-5e4138999ac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-20 14:21:34 [INFO]: Using the given device: cuda\n",
            "2025-06-20 14:21:34 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2025-06-20 14:21:34 [INFO]: Using customized CrossEntropy as the training loss function.\n",
            "2025-06-20 14:21:34 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
            "2025-06-20 14:21:34 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 4,389,832\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Epoch 1/5 — 615 batches\n",
            "  Batch 615/615 — loss: 1.2209 — acc: 53.09%\n",
            "Epoch 1 completed — avg train loss: 1.3994 — avg train acc: 45.70%\n",
            "\n",
            "Epoch 2/5 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1236 — acc: 60.53%\n",
            "Epoch 2 completed — avg train loss: 1.1868 — avg train acc: 55.45%\n",
            "\n",
            "Epoch 3/5 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1297 — acc: 59.38%\n",
            "Epoch 3 completed — avg train loss: 1.1343 — avg train acc: 57.68%\n",
            "\n",
            "Epoch 4/5 — 615 batches\n",
            "  Batch 615/615 — loss: 1.0968 — acc: 60.30%\n",
            "Epoch 4 completed — avg train loss: 1.1010 — avg train acc: 58.96%\n",
            "\n",
            "Epoch 5/5 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1281 — acc: 62.01%\n",
            "Epoch 5 completed — avg train loss: 1.0721 — avg train acc: 60.15%\n",
            "\n",
            "Evaluating on test set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5698    0.6552    0.6096     18094\n",
            "           1     0.7330    0.8139    0.7713     17838\n",
            "           2     0.8738    0.9022    0.8878      5359\n",
            "           3     0.8008    0.6684    0.7287     15731\n",
            "           4     0.6202    0.7399    0.6748     21526\n",
            "           5     0.4266    0.4957    0.4586     17993\n",
            "           6     0.5170    0.4682    0.4914     21399\n",
            "           7     0.4800    0.2735    0.3485     16978\n",
            "\n",
            "    accuracy                         0.6021    134918\n",
            "   macro avg     0.6277    0.6271    0.6213    134918\n",
            "weighted avg     0.5997    0.6021    0.5946    134918\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Imports ===\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PyPOTS TimesNet import\n",
        "from pypots.classification.timesnet.model import TimesNet\n",
        "\n",
        "\n",
        "# === DataLoaders ===\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "# === Model, Optimizer, and Loss ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "n_steps, n_features = X_tensor.shape[1], X_tensor.shape[2]\n",
        "n_classes = int(torch.unique(y_tensor).numel())\n",
        "\n",
        "model = TimesNet(\n",
        "    n_steps=n_steps,\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    n_layers=3,\n",
        "    top_k=3,\n",
        "    d_model=64,\n",
        "    d_ffn=128,\n",
        "    n_kernels=4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = Adam(model.model.parameters(), lr=1e-4)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# === Training Loop (with NaN-guards + batch-skip) ===\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.model.train()\n",
        "    total_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    num_batches = len(train_loader)\n",
        "    print(f\"\\nEpoch {epoch}/{epochs} — {num_batches} batches\")\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(train_loader, start=1):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # guard again per-batch just in case\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        if logits is None:\n",
        "            raise RuntimeError(\"Expected 'logits' in TimesNet output for proper loss computation.\")\n",
        "\n",
        "        # wipe any NaNs/Infs in logits\n",
        "        logits = logits.nan_to_num(nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nWarning: loss=NaN at batch {batch_idx}, skipping this batch\")\n",
        "            continue\n",
        "\n",
        "        # per-batch accuracy\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == yb).sum().item()\n",
        "        batch_acc = correct / yb.size(0)\n",
        "\n",
        "        epoch_correct += correct\n",
        "        epoch_total += yb.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"  Batch {batch_idx}/{num_batches} — loss: {loss.item():.4f} — acc: {batch_acc*100:.2f}%\", end='\\r')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = epoch_correct / epoch_total if epoch_total>0 else 0\n",
        "    print(f\"\\nEpoch {epoch} completed — avg train loss: {avg_loss:.4f} — avg train acc: {avg_acc*100:.2f}%\")\n",
        "\n",
        "# === Evaluation on Test Set ===\n",
        "model.model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "num_test_batches = len(test_loader)\n",
        "print(f\"\\nEvaluating on test set — {num_test_batches} batches\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (xb, yb) in enumerate(test_loader, start=1):\n",
        "        xb = xb.to(device)\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        probs = outputs[\"classification_proba\"].nan_to_num(nan=1e-9, posinf=1.0, neginf=0.0)\n",
        "        preds = probs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "        print(f\"  Test batch {batch_idx}/{num_test_batches}\", end='\\r')\n",
        "\n",
        "print(\"\\n\\n=== Classification Report ===\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IeUuCYll6nbE",
      "metadata": {
        "id": "IeUuCYll6nbE"
      },
      "source": [
        "# **Dodane wag na klasę** (w zależności od F1 per klasa uzyskane powyżej) oraz zwiększenie epok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5b9d88-d3d7-4889-a0c0-5262d4c03ac4",
      "metadata": {
        "id": "6b5b9d88-d3d7-4889-a0c0-5262d4c03ac4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afa79c1",
      "metadata": {
        "id": "6afa79c1",
        "outputId": "d38542a0-1fa6-4c66-c897-fdb5aa2d92e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-20 16:32:43 [INFO]: Using the given device: cuda\n",
            "2025-06-20 16:32:43 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2025-06-20 16:32:43 [INFO]: Using customized CrossEntropy as the training loss function.\n",
            "2025-06-20 16:32:43 [INFO]: Using customized CrossEntropy as the validation metric function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-20 16:32:43 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 4,389,832\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.2658 — acc: 51.49%\n",
            "Epoch 1 completed — avg train loss: 1.4336 — avg train acc: 43.94%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4949    0.6430    0.5593     18467\n",
            "           1     0.7447    0.6748    0.7080     17965\n",
            "           2     0.8577    0.8494    0.8536      5466\n",
            "           3     0.7166    0.6215    0.6657     15283\n",
            "           4     0.6056    0.5401    0.5710     21682\n",
            "           5     0.3818    0.3510    0.3658     18129\n",
            "           6     0.4696    0.2669    0.3403     21148\n",
            "           7     0.3126    0.5209    0.3907     16776\n",
            "\n",
            "    accuracy                         0.5233    134916\n",
            "   macro avg     0.5730    0.5585    0.5568    134916\n",
            "weighted avg     0.5439    0.5233    0.5237    134916\n",
            "\n",
            "\n",
            "Epoch 2/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.2532 — acc: 53.66%\n",
            "Epoch 2 completed — avg train loss: 1.2425 — avg train acc: 54.53%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5453    0.6571    0.5960     18467\n",
            "           1     0.7848    0.6667    0.7209     17965\n",
            "           2     0.8836    0.8454    0.8641      5466\n",
            "           3     0.7142    0.6906    0.7022     15283\n",
            "           4     0.7004    0.5098    0.5901     21682\n",
            "           5     0.3912    0.5077    0.4419     18129\n",
            "           6     0.5047    0.2519    0.3361     21148\n",
            "           7     0.3450    0.5662    0.4287     16776\n",
            "\n",
            "    accuracy                         0.5512    134916\n",
            "   macro avg     0.6087    0.5869    0.5850    134916\n",
            "weighted avg     0.5830    0.5512    0.5523    134916\n",
            "\n",
            "\n",
            "Epoch 3/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.2254 — acc: 57.21%\n",
            "Epoch 3 completed — avg train loss: 1.1941 — avg train acc: 56.87%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5871    0.6077    0.5972     18467\n",
            "           1     0.7573    0.7525    0.7549     17965\n",
            "           2     0.8409    0.9087    0.8735      5466\n",
            "           3     0.7812    0.6567    0.7136     15283\n",
            "           4     0.6631    0.6177    0.6396     21682\n",
            "           5     0.3827    0.5381    0.4473     18129\n",
            "           6     0.5339    0.3813    0.4449     21148\n",
            "           7     0.4036    0.4427    0.4222     16776\n",
            "\n",
            "    accuracy                         0.5810    134916\n",
            "   macro avg     0.6187    0.6132    0.6117    134916\n",
            "weighted avg     0.5956    0.5810    0.5836    134916\n",
            "\n",
            "\n",
            "Epoch 4/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1816 — acc: 57.32%\n",
            "Epoch 4 completed — avg train loss: 1.1602 — avg train acc: 58.38%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5274    0.7114    0.6057     18467\n",
            "           1     0.7263    0.8072    0.7646     17965\n",
            "           2     0.9240    0.8225    0.8703      5466\n",
            "           3     0.7919    0.6554    0.7172     15283\n",
            "           4     0.7788    0.5372    0.6358     21682\n",
            "           5     0.4846    0.3648    0.4163     18129\n",
            "           6     0.4727    0.4235    0.4468     21148\n",
            "           7     0.3606    0.5368    0.4314     16776\n",
            "\n",
            "    accuracy                         0.5809    134916\n",
            "   macro avg     0.6333    0.6074    0.6110    134916\n",
            "weighted avg     0.6052    0.5809    0.5830    134916\n",
            "\n",
            "\n",
            "Epoch 5/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1294 — acc: 60.41%\n",
            "Epoch 5 completed — avg train loss: 1.1315 — avg train acc: 59.48%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5726    0.6746    0.6194     18467\n",
            "           1     0.8386    0.6197    0.7127     17965\n",
            "           2     0.8624    0.9069    0.8841      5466\n",
            "           3     0.6643    0.8004    0.7260     15283\n",
            "           4     0.7272    0.6212    0.6700     21682\n",
            "           5     0.4879    0.4009    0.4402     18129\n",
            "           6     0.4639    0.5510    0.5037     21148\n",
            "           7     0.4093    0.4195    0.4143     16776\n",
            "\n",
            "    accuracy                         0.5945    134916\n",
            "   macro avg     0.6283    0.6243    0.6213    134916\n",
            "weighted avg     0.6063    0.5945    0.5951    134916\n",
            "\n",
            "\n",
            "Epoch 6/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1343 — acc: 58.70%\n",
            "Epoch 6 completed — avg train loss: 1.1066 — avg train acc: 60.35%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5620    0.6777    0.6145     18467\n",
            "           1     0.8013    0.7325    0.7653     17965\n",
            "           2     0.8812    0.9049    0.8929      5466\n",
            "           3     0.7449    0.7262    0.7354     15283\n",
            "           4     0.7576    0.5786    0.6561     21682\n",
            "           5     0.4677    0.4695    0.4686     18129\n",
            "           6     0.4905    0.4100    0.4466     21148\n",
            "           7     0.3813    0.5290    0.4432     16776\n",
            "\n",
            "    accuracy                         0.5953    134916\n",
            "   macro avg     0.6358    0.6285    0.6278    134916\n",
            "weighted avg     0.6126    0.5953    0.5990    134916\n",
            "\n",
            "\n",
            "Epoch 7/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.0968 — acc: 60.76%\n",
            "Epoch 7 completed — avg train loss: 1.0773 — avg train acc: 61.45%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6498    0.6316    0.6405     18467\n",
            "           1     0.7392    0.8219    0.7784     17965\n",
            "           2     0.9368    0.8379    0.8846      5466\n",
            "           3     0.7789    0.6981    0.7363     15283\n",
            "           4     0.7078    0.7211    0.7144     21682\n",
            "           5     0.4961    0.4689    0.4821     18129\n",
            "           6     0.5256    0.4273    0.4713     21148\n",
            "           7     0.3922    0.5141    0.4449     16776\n",
            "\n",
            "    accuracy                         0.6187    134916\n",
            "   macro avg     0.6533    0.6401    0.6441    134916\n",
            "weighted avg     0.6251    0.6187    0.6194    134916\n",
            "\n",
            "\n",
            "Epoch 8/15 — 615 batches\n",
            "  Batch 615/615 — loss: 1.0272 — acc: 65.10%\n",
            "Epoch 8 completed — avg train loss: 1.0517 — avg train acc: 62.39%\n",
            "\n",
            "Evaluating on Val set — 132 batches\n",
            "  Test batch 132/132\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7138    0.5823    0.6414     18467\n",
            "           1     0.7979    0.7576    0.7773     17965\n",
            "           2     0.8800    0.9071    0.8933      5466\n",
            "           3     0.7502    0.7551    0.7526     15283\n",
            "           4     0.6510    0.7762    0.7081     21682\n",
            "           5     0.4422    0.6092    0.5125     18129\n",
            "           6     0.5693    0.4147    0.4799     21148\n",
            "           7     0.4598    0.4260    0.4422     16776\n",
            "\n",
            "    accuracy                         0.6275    134916\n",
            "   macro avg     0.6580    0.6535    0.6509    134916\n",
            "weighted avg     0.6350    0.6275    0.6256    134916\n",
            "\n",
            "\n",
            "Epoch 9/15 — 615 batches\n",
            "  Batch 306/615 — loss: 1.0322 — acc: 61.04%\r"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PyPOTS TimesNet import\n",
        "from pypots.classification.timesnet.model import TimesNet\n",
        "\n",
        "\n",
        "# === DataLoaders ===\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "# === Compute class-weights from F1-scores ===\n",
        "# F1-scores from last report, in class order 0..7\n",
        "f1_scores = np.array([0.6096, 0.7713, 0.8878, 0.7287, 0.6748, 0.4586, 0.4914, 0.3485], dtype=np.float32)\n",
        "# Inverse-F1 weighting: lower-F1 classes get higher weight\n",
        "inv_f1 = 1.0 / (f1_scores + 1e-6)  # avoid division by zero\n",
        "\n",
        "# === Model, Optimizer, and Loss ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "# class_weights on the correct device\n",
        "class_weights = torch.tensor(inv_f1, device=device)\n",
        "\n",
        "n_steps, n_features = X_tensor.shape[1], X_tensor.shape[2]\n",
        "n_classes = int(torch.unique(y_tensor).numel())\n",
        "\n",
        "model = TimesNet(\n",
        "    n_steps=n_steps,\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    n_layers=3,\n",
        "    top_k=3,\n",
        "    d_model=64,\n",
        "    d_ffn=128,\n",
        "    n_kernels=4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# drop lr to 1e-4 to stabilize\n",
        "optimizer = Adam(model.model.parameters(), lr=1e-4)\n",
        "# pass class_weights into the loss\n",
        "criterion = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# === Training Loop (with NaN-guards + batch-skip) ===\n",
        "epochs = 15\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.model.train()\n",
        "    total_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    num_batches = len(train_loader)\n",
        "    print(f\"\\nEpoch {epoch}/{epochs} — {num_batches} batches\")\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(train_loader, start=1):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        # guard again per-batch just in case\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        if logits is None:\n",
        "            raise RuntimeError(\"Expected 'logits' in TimesNet output for proper loss computation.\")\n",
        "\n",
        "        # wipe any NaNs/Infs in logits\n",
        "        logits = logits.nan_to_num(nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nWarning: loss=NaN at batch {batch_idx}, skipping this batch\")\n",
        "            continue\n",
        "\n",
        "        # per-batch accuracy\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == yb).sum().item()\n",
        "        batch_acc = correct / yb.size(0)\n",
        "\n",
        "        epoch_correct += correct\n",
        "        epoch_total += yb.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"  Batch {batch_idx}/{num_batches} — loss: {loss.item():.4f} — acc: {batch_acc*100:.2f}%\", end='\\r')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = epoch_correct / epoch_total if epoch_total>0 else 0\n",
        "    print(f\"\\nEpoch {epoch} completed — avg train loss: {avg_loss:.4f} — avg train acc: {avg_acc*100:.2f}%\")\n",
        "\n",
        "    # === Evaluation on Val Set ===\n",
        "    model.model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    num_test_batches = len(val_loader)\n",
        "    print(f\"\\nEvaluating on Val set — {num_test_batches} batches\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xb, yb) in enumerate(val_loader, start=1):\n",
        "            xb = xb.to(device)\n",
        "            xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            outputs = model.model({\"X\": xb})\n",
        "            probs = outputs[\"classification_proba\"].nan_to_num(nan=1e-9, posinf=1.0, neginf=0.0)\n",
        "            preds = probs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "            print(f\"  Test batch {batch_idx}/{num_test_batches}\", end='\\r')\n",
        "\n",
        "    print(\"\\n\\n=== Classification Report ===\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# === Evaluation on Test Set ===\n",
        "model.model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "num_test_batches = len(test_loader)\n",
        "print(f\"\\nEvaluating on test set — {num_test_batches} batches\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (xb, yb) in enumerate(test_loader, start=1):\n",
        "        xb = xb.to(device)\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        probs = outputs[\"classification_proba\"].nan_to_num(nan=1e-9, posinf=1.0, neginf=0.0)\n",
        "        preds = probs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(yb.numpy())\n",
        "        print(f\"  Test batch {batch_idx}/{num_test_batches}\", end='\\r')\n",
        "\n",
        "print(\"\\n\\n=== Classification Report ===\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c32360",
      "metadata": {},
      "outputs": [],
      "source": [
        "# After collecting all_preds and all_labels from test_loader...\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot it\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Test Set Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LN20IGRR5vYh",
      "metadata": {
        "id": "LN20IGRR5vYh"
      },
      "source": [
        "#Dodane wag na klasy oraz **zwiększenie parametrów TimesNet** (z 4 do 22 milionów)\n",
        "\n",
        "Czas na epokę: ok. 30 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68294504-af6d-43fb-b193-57b236ed6702",
      "metadata": {
        "id": "68294504-af6d-43fb-b193-57b236ed6702",
        "outputId": "77598566-1c25-40f4-dead-3d85d38c885b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\n",
            "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
            "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
            "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
            "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
            "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
            "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
            "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
            "\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-20 15:18:49 [INFO]: Using the given device: cuda\n",
            "2025-06-20 15:18:49 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
            "2025-06-20 15:18:49 [INFO]: Using customized CrossEntropy as the training loss function.\n",
            "2025-06-20 15:18:49 [INFO]: Using customized CrossEntropy as the validation metric function.\n",
            "2025-06-20 15:18:50 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 22,543,752\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/2 — 615 batches\n",
            "  Batch 615/615 — loss: 1.2713 — acc: 52.17%\n",
            "Epoch 1 completed — avg train loss: 1.3842 — avg train acc: 46.80%\n",
            "\n",
            "Epoch 2/2 — 615 batches\n",
            "  Batch 615/615 — loss: 1.1423 — acc: 58.01%\n",
            "Epoch 2 completed — avg train loss: 1.2039 — avg train acc: 56.05%\n",
            "\n",
            "Evaluating on Validation set — 132 batches\n",
            "  Validation batch 79/132\r"
          ]
        }
      ],
      "source": [
        "# === Imports ===\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from pypots.classification.timesnet.model import TimesNet\n",
        "\n",
        "\n",
        "# === Derive class-weights from F1-scores ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# F1-scores from last report, in class order 0..7\n",
        "f1_scores = np.array([0.6096, 0.7713, 0.8878, 0.7287, 0.6748, 0.4586, 0.4914, 0.3485], dtype=np.float32)\n",
        "\n",
        "# Inverse-F1 weighting: lower-F1 classes get higher weight\n",
        "inv_f1 = 1.0 / (f1_scores + 1e-6)       # avoid division by zero\n",
        "class_weights = torch.tensor(inv_f1, device=device)\n",
        "\n",
        "\n",
        "# === DataLoaders ===\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "# === Cell 5: Model, Optimizer, and Weighted Loss ===\n",
        "n_steps, n_features = X_tensor.shape[1], X_tensor.shape[2]\n",
        "n_classes = int(torch.unique(y_tensor).numel())\n",
        "\n",
        "model = TimesNet(\n",
        "    n_steps=n_steps,\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    n_layers=4,      # you can tune deeper/shallower\n",
        "    top_k=3,\n",
        "    d_model=128,     # embedding dimension\n",
        "    d_ffn=256,       # feed-forward width\n",
        "    n_kernels=4,\n",
        "    device=device\n",
        ")\n",
        "# NOTE: TimesNet already places its .model on `device`\n",
        "\n",
        "# Lower LR and small weight decay\n",
        "optimizer = Adam(model.model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Pass our F1-based class_weights into the loss\n",
        "criterion = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "\n",
        "# === Cell 6: Training Loop with full logging ===\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.model.train()\n",
        "    total_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    num_batches = len(train_loader)\n",
        "    print(f\"\\nEpoch {epoch}/{epochs} — {num_batches} batches\")\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(train_loader, start=1):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        logits = logits.nan_to_num(nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nWarning: loss=NaN at batch {batch_idx}, skipping\")\n",
        "            continue\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == yb).sum().item()\n",
        "        batch_acc = correct / yb.size(0)\n",
        "\n",
        "        epoch_correct += correct\n",
        "        epoch_total += yb.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # per-batch print\n",
        "        print(f\"  Batch {batch_idx}/{num_batches} — loss: {loss.item():.4f} — acc: {batch_acc*100:.2f}%\", end='\\r')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = epoch_correct / epoch_total * 100\n",
        "    print(f\"\\nEpoch {epoch} completed — avg train loss: {avg_loss:.4f} — avg train acc: {avg_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# === Cell 7: Evaluation on Validation & Test Sets ===\n",
        "for split_name, loader in [(\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
        "    model.model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    num_batches = len(loader)\n",
        "    print(f\"\\nEvaluating on {split_name} set — {num_batches} batches\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xb, yb) in enumerate(loader, start=1):\n",
        "            xb = xb.to(device).nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            outputs = model.model({\"X\": xb})\n",
        "            probs = outputs[\"classification_proba\"].nan_to_num(nan=1e-9, posinf=1.0, neginf=0.0)\n",
        "            preds = probs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "            print(f\"  {split_name} batch {batch_idx}/{num_batches}\", end='\\r')\n",
        "\n",
        "    print(f\"\\n\\n=== {split_name} Classification Report ===\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "    print(f\"=== {split_name} Confusion Matrix ===\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    sns.heatmap(cm, fmt='d', annot=False)\n",
        "    plt.title(f\"{split_name} Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7496c2f-21a3-4fcf-937e-6b0269d0c0bc",
      "metadata": {
        "id": "c7496c2f-21a3-4fcf-937e-6b0269d0c0bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2594cb08-d59d-4ed4-b622-2de445163223",
      "metadata": {
        "id": "2594cb08-d59d-4ed4-b622-2de445163223"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3e98584a",
      "metadata": {},
      "source": [
        "# Zapisywanie modelu z pamięci podręcznej"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2518c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.model.state_dict(), \"timesnet_state_dict.pth\")\n",
        "print(\"Model state_dict saved to 'timesnet_state_dict.pth'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abc32a1-961d-4609-99ce-d1aca16bacc0",
      "metadata": {
        "id": "3abc32a1-961d-4609-99ce-d1aca16bacc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b61c52-bad3-4c2e-bed4-0b5e7ddeb1a7",
      "metadata": {
        "id": "64b61c52-bad3-4c2e-bed4-0b5e7ddeb1a7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "gdhZJz3t5bLm",
      "metadata": {
        "id": "gdhZJz3t5bLm"
      },
      "source": [
        "# Dodanie samplera, tak aby ładował więcej małolicznych klas\n",
        "\n",
        "Z tym że tutaj może trzeba będzie zmienić to, aby ładował w zależności od wag czyli na bazie F1, ponieważ mało liczne =/= niskie F1, najmniej liczna klasa 3 bodajże ma najwyższe F1, bo jest najbardziej charakterystyczna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3587738-c53e-482d-943e-7bf331d64128",
      "metadata": {
        "id": "d3587738-c53e-482d-943e-7bf331d64128"
      },
      "outputs": [],
      "source": [
        "# === Imports ===\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from pypots.classification.timesnet.model import TimesNet\n",
        "\n",
        "\n",
        "# === Derive class-weights from F1-scores ===\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# F1-scores from last report, in class order 0..7\n",
        "f1_scores = np.array([0.5491, 0.7192, 0.8305, 0.6749, 0.5815, 0.3646, 0.4070, 0.3302], dtype=np.float32)\n",
        "\n",
        "# Inverse-F1 weighting: lower-F1 classes get higher weight\n",
        "inv_f1 = 1.0 / (f1_scores + 1e-6)       # avoid division by zero\n",
        "class_weights = torch.tensor(inv_f1, device=device)\n",
        "\n",
        "# Build a sampler that oversamples based on these weights\n",
        "y_all = y_tensor.cpu().numpy()\n",
        "train_indices = train_ds.indices                 # indices into full_ds\n",
        "train_labels  = y_all[train_indices]             # array of shape (n_train,)\n",
        "sample_weights = inv_f1[train_labels]            # weight for each example\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "\n",
        "# === DataLoaders ===\n",
        "batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler,    # oversampling minority classes\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "# === Model, Optimizer, and Weighted Loss ===\n",
        "n_steps, n_features = X_tensor.shape[1], X_tensor.shape[2]\n",
        "n_classes = int(torch.unique(y_tensor).numel())\n",
        "\n",
        "model = TimesNet(\n",
        "    n_steps=n_steps,\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    n_layers=4,      # can tune deeper/shallower\n",
        "    top_k=3,\n",
        "    d_model=128,     # embedding dimension\n",
        "    d_ffn=256,       # feed-forward width\n",
        "    n_kernels=4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "optimizer = Adam(model.model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Pass our F1-based class_weights into the loss\n",
        "criterion = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "\n",
        "# === Training Loop with full logging ===\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.model.train()\n",
        "    total_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    epoch_total = 0\n",
        "    num_batches = len(train_loader)\n",
        "    print(f\"\\nEpoch {epoch}/{epochs} — {num_batches} batches\")\n",
        "\n",
        "    for batch_idx, (xb, yb) in enumerate(train_loader, start=1):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        xb = xb.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        outputs = model.model({\"X\": xb})\n",
        "        logits = outputs.get(\"logits\")\n",
        "        logits = logits.nan_to_num(nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"\\nWarning: loss=NaN at batch {batch_idx}, skipping\")\n",
        "            continue\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct = (preds == yb).sum().item()\n",
        "        batch_acc = correct / yb.size(0)\n",
        "\n",
        "        epoch_correct += correct\n",
        "        epoch_total += yb.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # per-batch print\n",
        "        print(f\"  Batch {batch_idx}/{num_batches} — loss: {loss.item():.4f} — acc: {batch_acc*100:.2f}%\", end='\\r')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_acc = epoch_correct / epoch_total * 100\n",
        "    print(f\"\\nEpoch {epoch} completed — avg train loss: {avg_loss:.4f} — avg train acc: {avg_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# === Evaluation on Validation & Test Sets ===\n",
        "for split_name, loader in [(\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
        "    model.model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    num_batches = len(loader)\n",
        "    print(f\"\\nEvaluating on {split_name} set — {num_batches} batches\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xb, yb) in enumerate(loader, start=1):\n",
        "            xb = xb.to(device).nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            outputs = model.model({\"X\": xb})\n",
        "            probs = outputs[\"classification_proba\"].nan_to_num(nan=1e-9, posinf=1.0, neginf=0.0)\n",
        "            preds = probs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(yb.numpy())\n",
        "            print(f\"  {split_name} batch {batch_idx}/{num_batches}\", end='\\r')\n",
        "\n",
        "    print(f\"\\n\\n=== {split_name} Classification Report ===\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "    print(f\"=== {split_name} Confusion Matrix ===\")\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    sns.heatmap(cm, fmt='d', annot=False)\n",
        "    plt.title(f\"{split_name} Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95b1e15-f5a6-4e86-b038-cd122246111b",
      "metadata": {
        "id": "c95b1e15-f5a6-4e86-b038-cd122246111b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bc0NXv1U8eOo",
      "metadata": {
        "id": "bc0NXv1U8eOo"
      },
      "source": [
        "Zaimplementować należy optymalne LR, z sensownym degradacją decay, zmniejszaniem LR z numerem epoki i spadkiem Lossu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5BlOq-Vx61Vh",
      "metadata": {
        "id": "5BlOq-Vx61Vh"
      },
      "source": [
        "# **Implementacja Augmentacji**\n",
        "\n",
        "Przykładowo:\n",
        "\n",
        "Dla sygnałów transformsy: jittering (adding small noise), scaling, time-warping, or permutation\n",
        "\n",
        "poniżej załączony przykłądowy kod, ale może istnieją biblioteki które to robią, napewno zwiększy to czas uczenia z oczekiwanych 30 min na epokę, do prawdopodobnie nawet godziny na epokę.\n",
        "\n",
        "***Trzeba to zacząć implementację ASAP.***\n",
        "\n",
        "\n",
        "```\n",
        "import random\n",
        "\n",
        "# === Augmentation functions ===\n",
        "def jitter(x, σ=0.03):\n",
        "    noise = torch.randn_like(x) * σ\n",
        "    return x + noise\n",
        "\n",
        "def scaling(x, σ=0.1):\n",
        "    factor = torch.randn(x.size(0), 1, x.size(2), device=x.device) * σ + 1.0\n",
        "    return x * factor\n",
        "\n",
        "def permutation(x, max_segments=5):\n",
        "    orig = x.clone().cpu().numpy()\n",
        "    B, T, F = orig.shape\n",
        "    permuted = []\n",
        "    for b in range(B):\n",
        "        n_segs = random.randint(1, max_segments)\n",
        "        splits = np.array_split(np.arange(T), n_segs)\n",
        "        random.shuffle(splits)\n",
        "        permuted.append(np.concatenate(splits, axis=0))\n",
        "    idx = torch.from_numpy(np.stack(permuted)).to(x.device)\n",
        "    return x[torch.arange(B).unsqueeze(1), idx]\n",
        "\n",
        "# === Custom Dataset with Augmentation ===\n",
        "class AugmentedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tensor_ds, augment=True):\n",
        "        self.ds = tensor_ds\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.ds[idx]\n",
        "        if self.augment:\n",
        "            # randomly pick one or more transforms\n",
        "            if random.random() < 0.5:\n",
        "                x = jitter(x)\n",
        "            if random.random() < 0.5:\n",
        "                x = scaling(x)\n",
        "            if random.random() < 0.2:\n",
        "                x = permutation(x)\n",
        "        return x, y\n",
        "\n",
        "# === Replace your train_ds/DataLoader with augmented version ===\n",
        "aug_train_ds = AugmentedDataset(train_ds, augment=True)\n",
        "train_loader = DataLoader(\n",
        "    aug_train_ds,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler,    # keep your weighted sampler\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# val_loader/test_loader stay unchanged (no augmentation)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05586b11-fa19-4902-aca4-341a0dfc0de2",
      "metadata": {
        "id": "05586b11-fa19-4902-aca4-341a0dfc0de2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a64d4f3-52d5-4aad-bdb8-d91c71bd08c4",
      "metadata": {
        "id": "5a64d4f3-52d5-4aad-bdb8-d91c71bd08c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6278629b",
      "metadata": {
        "id": "6278629b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4852dd9e",
      "metadata": {
        "id": "4852dd9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c64d33",
      "metadata": {
        "id": "25c64d33"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
